{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit ('nn': conda)",
      "language": "python",
      "name": "python_defaultSpec_1599604510628"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0-final"
    },
    "colab": {
      "name": "LS_DS17_422_Train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS4GZ37Wgcjr",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 2, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "etFf1WLWgcjt",
        "colab_type": "text"
      },
      "source": [
        "# Train (Prepare)\n",
        "__*Neural Network Foundations*__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXB80QOhgcju",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Student should be able to explain the intuition behind backpropagation and gradient descent\n",
        "* <a href=\"#p2\">Part 2</a>: Student should be able to discuss the importance of batch size\n",
        "* <a href=\"#p3\">Part 3</a>: Student should be able to discuss the importance of learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YuQu2lfgcju",
        "colab_type": "text"
      },
      "source": [
        "## Summary of Yesterday\n",
        "\n",
        "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
        "\n",
        "  <center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
        "\n",
        "- There are three kinds of layers: input, hidden, and output layers.\n",
        "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
        "- Signal is passed from layer to layer through a network by:\n",
        " - Taking in inputs from the training data (or previous layer)\n",
        " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
        " - Adding a bias to this weighted some of inputs and weights\n",
        " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
        "\\begin{align}\n",
        " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
        "\\end{align}\n",
        " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi4R03rgcjv",
        "colab_type": "text"
      },
      "source": [
        "## Training a Neural Network: *Formal Summary*\n",
        "\n",
        "0. Pick a network architecture\n",
        "   - No. of input units = No. of features\n",
        "   - No. of output units = Number of Classes (or expected targets)\n",
        "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
        "1. Randomly initialize weights\n",
        "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
        "3. Implement code to compute a cost function $J(\\theta)$\n",
        "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
        "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
        "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aM4CK1IarId4",
        "toc-hr-collapsed": false
      },
      "source": [
        "# Backpropagation & Gradient Descent (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "toc-hr-collapsed": true,
        "id": "Ktm8Fmoagcjy"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXI2tEO9gcjy",
        "colab_type": "text"
      },
      "source": [
        "### What is a Gradient?\n",
        "\n",
        "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
        "\n",
        "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "UZY66kiUgcjz",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "In this section, we will again a simple neural network using base TensorFlow. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dm2HPETcrgy6",
        "toc-hr-collapsed": true
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_99C94113202D-1.jpeg\"width=500></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4d4tzpwO6B47"
      },
      "source": [
        "### Generate some Fake Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ERyVgeO_IWyV",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Imagine that our data is drawn from a linear function\n",
        "# y = 3*hours_studying + 50\n",
        "\n",
        "TRUE_W = 3.5\n",
        "TRUE_b = 50.0\n",
        "NUM_EXAMPLES = 1000\n",
        "\n",
        "inputs = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "outputs = inputs * TRUE_W + TRUE_b + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2khtdq0uoI_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e3051d17-1078-4f85-ed69-4a7128834507"
      },
      "source": [
        "inputs[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([ 0.20828933, -1.5472957 , -1.3575413 , -1.3042835 , -0.82876664],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sCoajXKoPmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "24b276c3-5e4e-470f-fab0-500baa19137d"
      },
      "source": [
        "noise[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([-0.5452991 , -0.81521344,  1.9958404 , -0.80046517,  0.5429737 ],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSCxwl_toRDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "57c70029-26de-4f75-a321-9a179c0541f0"
      },
      "source": [
        "outputs[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([50.183712, 43.769253, 47.244446, 44.634544, 47.64229 ],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJesGEUgcj4",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function\n",
        "Here we will use Mean Squared Error (MSE), because this is a regression problem. We are trying to predict a continuous target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cDeUBW6k4Ri4",
        "colab": {}
      },
      "source": [
        "def loss(target_y, predicted_y):\n",
        "  \"MSE\"\n",
        "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bgTf6vTS69Sw"
      },
      "source": [
        "### Neural Network Architecture\n",
        "Lets create a Neural Network class called \"Model\" to contain this functionality. Note: This is essentially a linear regression whose coefficients are trained by gradient descent. In practice, gradient descent works on much more complex function like the multi-layer networks we constructed yesterday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUI8VSR5zyBv",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = tf.Variable(8.0)\n",
        "    self.b = tf.Variable(40.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.W * x + self.b\n",
        "\n",
        "model = Model()\n",
        "\n",
        "assert model(3.0).numpy() == 64.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAci_dIjpCIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca8f3b04-0423-484b-87a9-d29b662471ef"
      },
      "source": [
        "model(3.0).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gbyT_FJ88IlK"
      },
      "source": [
        "### Initial Weights\n",
        "The initial weights in our model were arbitrary. In practice, weights are initialized randomly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IreIDe6P8H0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "0bd77b35-5f21-41a6-d6d2-22dde8ce534b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(inputs, outputs, c='b')\n",
        "plt.scatter(inputs, model(inputs), c='r')\n",
        "plt.show()\n",
        "\n",
        "print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Current loss: 123.947189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "16Ujj6vNYQyX",
        "toc-hr-collapsed": true
      },
      "source": [
        "### Update Weights Based on Gradient\n",
        "\n",
        "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
        "\n",
        "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). \n",
        "\n",
        "Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
        "\n",
        "1) Stochastic Gradient Descent\n",
        "\n",
        "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgaGD6YlHoid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t: \n",
        "     current_loss = loss(outputs, model(inputs))\n",
        "  dW, db = t.gradient(current_loss, [model.W, model.b])\n",
        "  model.W.assign_sub(learning_rate * dW)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iziWWURgck8",
        "colab_type": "text"
      },
      "source": [
        "### Train the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zn_HgFuHhTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "31a92590-508c-408a-f405-ce7eb82d46e8"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# Store Some history of weights\n",
        "Ws, bs = [], []\n",
        "epochs = range(10)\n",
        "for epoch in epochs:\n",
        "  Ws.append(model.W.numpy())\n",
        "  bs.append(model.b.numpy())\n",
        "  current_loss = loss(outputs, model(inputs))\n",
        "\n",
        "  train(model, inputs, outputs, learning_rate=0.1)\n",
        "  print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0: W=8.00 b=40.00 loss=123.94719\n",
            "Epoch  1: W=7.07 b=42.03 loss=79.01660\n",
            "Epoch  2: W=6.32 b=43.65 loss=50.49422\n",
            "Epoch  3: W=5.73 b=44.94 loss=32.38786\n",
            "Epoch  4: W=5.26 b=45.97 loss=20.89367\n",
            "Epoch  5: W=4.89 b=46.79 loss=13.59695\n",
            "Epoch  6: W=4.59 b=47.45 loss=8.96485\n",
            "Epoch  7: W=4.36 b=47.97 loss=6.02427\n",
            "Epoch  8: W=4.17 b=48.38 loss=4.15752\n",
            "Epoch  9: W=4.02 b=48.71 loss=2.97245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSEt07wdHvi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "97194ab5-509f-4ef3-8bad-3cbea64d42e1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs, Ws, 'r', epochs, bs, 'b')\n",
        "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
        "         [TRUE_b] * len(epochs), 'b--')\n",
        "plt.legend(['W', 'b', 'True W', 'True b'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKUVGoRxgck_",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project, you will be asked to explain the logic of backpropagation and gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "vTqZg-6igclA",
        "colab_type": "text"
      },
      "source": [
        "# Batch Size (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrm-racgclA",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
        "\n",
        "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. \n",
        "\n",
        "### Batch Size\n",
        "Batches are the number of observations our model is shown to make predictions and update the weights. Batches are selected randomly during epoch. All observations are considered when passing thru an epoch at some point.\n",
        "\n",
        "* Smaller Batch = Slower Run Time (but maybe more accurate results)\n",
        "* Default Batch = Balance between speed and accuracy\n",
        "* Large Batch = Very fast, but not nearly as accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNQ2ZCi7I4i6",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjW2lYVI9Q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fc3ea0b1-e87d-4566-83b1-f8e815ec307c"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "X_train = X_train.reshape((60000, 784))\n",
        "X_test = X_test.reshape((10000, 784))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7x17kDKJSy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizer import SGD\n",
        "\n",
        "def create_model(lr=.01):\n",
        "  opt = SGD(learning_rate=lr)\n",
        "\n",
        "  model = Sequential(\n",
        "      [\n",
        "       Dense(32, activation='relu')\n",
        "       \n",
        "      ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZF7UE-KluPsX"
      },
      "source": [
        "## Follow Along\n",
        "Let's run a series of experiments for a default, small, and large batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhpDaVFRJl3U",
        "colab_type": "text"
      },
      "source": [
        "### Default\n",
        "Batch Size is 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-ChVGikgclD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvsbOFnDJuG0",
        "colab_type": "text"
      },
      "source": [
        "### Small Batch Size\n",
        "Batch Size is 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diDzvb-UJ1je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iPvvvt5J2Xl",
        "colab_type": "text"
      },
      "source": [
        "### Large Batch Size\n",
        "Batch Size is 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h8Z5293KABT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ujUz6BKUGz",
        "colab_type": "text"
      },
      "source": [
        "### Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5DOZNMKYt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZ2vUYYgclS",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with batch size on today's assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cP9Pm_gclS",
        "colab_type": "text"
      },
      "source": [
        "# Learning Rate (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "Bna67ADZgclT",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Learning Rate controls the size of the update to our weights that the optimization algorithm makes. VERY IMPORTANT hyperparameter.\n",
        "\n",
        "* Too high of a learning rate causes unstable results\n",
        "* Too Low of a learning rate the model will underfit\n",
        "* Goldy Locks parameters - it needs be \"just right\"\n",
        "* Scale of 0-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "gsVYOn7bgcle",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Same experiment with Batch but different learning rates:\n",
        "* High Learning = .75\n",
        "* Default Learning = .01\n",
        "* Low Learning Rate = .0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI_H8Em1NOii",
        "colab_type": "text"
      },
      "source": [
        "### Default Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8cb_ZUNVtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQZ4SZdKNMRO",
        "colab_type": "text"
      },
      "source": [
        "### High Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny72mU_dNWMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqDmTVBNSMR",
        "colab_type": "text"
      },
      "source": [
        "### Low Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ech1ER64NXBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZe6DyhANXdU",
        "colab_type": "text"
      },
      "source": [
        "### Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-BdFdMNph-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2aiw_Sgcl7",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with different learning rates today.\n",
        "\n",
        "---"
      ]
    }
  ]
}