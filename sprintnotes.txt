"BY NO MEANS COMPREHENSIVE" jcsokoll
 
api > tensorflow, keras 
tools & terminology
	
types of architectures, 
	 components
	how to piece together
 

foundational components of a neural network
implement a perceptron from scratch in python

threshold -> fire, activation
neuron -> node -> take in signal, pass out signal value


single node architecture
	networks are multinode

INPUT NODE for each FEATURE of dataset
OUTPUT NODE target
HIDDEN we don't inspect the values

different architecture for different problems
	PERCEPTRON is the simplest
		NO HIDDEN LAYERS
		 only INPUT and OUTPUT
	FF FEED FORWARD/multilayer perceptron
	DFF DEEP FEED FORWARD
	RNN RECURRENT NEURAL NETWORK
	LSTM LONG/SHORT TERM MEMORY
	AE AUTO-ENCODERS
next wk: GAN GENERAL ADVERSARIAL
	CNN CONVOLUTIONAL NEURAL NETWORKS

DENSE/FULLY-CONNECTED nodes input flows from one-to-all

input(visible), hidden, output layers
hidden -> inaccessible in training
deep -> lots of hidden layers
OUTPUT -> vector values that the model is supposed to predict > can vary
	REGRESSION -> single output node with no activation function
			because what we want is an UNBOUNDED CONTINOUS VALUE
	BINARY CLASSIFICATION -> single output WITH ACTIVATION FUNCT (sigmoid etc)
	MULTICLASS CLASSIFICATION -> output per class; probably SOFTMAX activation funct

architectures with no output (non-FF-type) -> for feature extraction or reccommendation

bias > initially we hardcoded, arbitrary;
	later in keras/tf, value is always 1, weight associated *garble* updated via backprop
ACTIVATION FUNCT - step was in original perceptron; relu is most common today



neural network
perceptron
feed forward
nodes
layers
input, hidden, output
bias
epoch
backpropagation
multilayer perceptron
neuron - simple computational unit; performs weighted sum on incoming signals, 
	adds a threshold or bias term to this value to yield a net-input, 
	and maps this last value through an activation function to compute it's own activation
	- some neurons (feedback, hopfield networks) retain portion of previous activation
threshold
weight
loss functions > options
	which is best for task at hand
	crossentropy
optimizers > options
	stochastic gradient descent (sgd) > adam, nadam

